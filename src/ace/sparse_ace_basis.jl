
using SparseArrays: SparseMatrixCSC
using LinearAlgebra: mul!
import ChainRulesCore: NoTangent, rrule, ZeroTangent
import LuxCore: AbstractLuxLayer, initialparameters, initialstates, apply 


struct SparseACEbasis{NL, TA, TAA, TSYM} <: AbstractLuxLayer
   abasis::TA
   aabasis::TAA
   A2Bmaps::TSYM
   LL::NTuple{NL, Int} 
   lens::NTuple{NL, Int} 
   # ---- 
   meta::Dict{String, Any}
end

function SparseACEbasis(abasis, aabasis, A2Bmaps, meta) 
   LL = []
   lens = [] 
   for i = 1:length(A2Bmaps)
      tLp1 = length(A2Bmaps[i][1])
      push!(LL, (tLp1 - 1) Ã· 2)
      push!(lens, size(A2Bmaps[i], 1))
   end
   SparseACEbasis(abasis, aabasis, A2Bmaps, 
             tuple(LL...), tuple(lens...), meta)
end

Base.length(tensor::SparseACEbasis) = sum(tensor.lens)

function Base.length(tensor::SparseACEbasis, L::Integer)
   for (il, l) in enumerate(tensor.LL)
      if l == L
         return tensor.lens[il]
      end
   end
   error("Layer does not have an for L = $L output")
end

function Base.show(io::IO, l::SparseACEbasis)
   print(io, "SparseACEbasis(L = $(l.LL))")
end


# ----------------------------------------
# Lux integration 

(l::SparseACEbasis)(BB::Tuple, ps, st) = evaluate(l, BB..., ps, st), st 

initialparameters(rng::AbstractRNG, bas::SparseACEbasis) = 
         NamedTuple() 

initialstates(rng::AbstractRNG, bas::SparseACEbasis) = 
         ( aspec = bas.abasis.spec, 
            aaspecs = bas.aabasis.specs, 
            A2Bmaps = SparseMatCSX.(bas.A2Bmaps), )


# ----------------------------------------
# evaluation kernels 

#=
function evaluate!(B, tensor::SparseACEbasis{T}, Rnl, Ylm) where {T}
   # evaluate the A basis
   TA = promote_type(eltype(Rnl), eltype(Ylm))
   A = zeros(TA, length(tensor.abasis))    # use Bumper here
   evaluate!(A, tensor.abasis, (Rnl, Ylm))

   # evaluate the AA basis
   AA = zeros(TA, length(tensor.aabasis))     # use Bumper here
   evaluate!(AA, tensor.aabasis, A)

   # evaluate the coupling coefficients
   # B = tensor.A2Bmap * AA

   mul!(B, tensor.A2Bmap, AA)   

   return B
end

function whatalloc(::typeof(evaluate!), tensor::SparseACEbasis, Rnl, Ylm)
   TA = promote_type(eltype(Rnl), eltype(Ylm))
   TB = _promote_mul_type(TA, eltype(tensor.A2Bmap))
   return TB, length(tensor)
end

=#

evaluate(tensor::SparseACEbasis, Rnl, Ylm) = 
      evaluate(tensor, Rnl, Ylm, NamedTuple(), NamedTuple()) 

#=
function evaluate(tensor::SparseACEbasis, Rnl, Ylm, ps, st)
   allocinfo = whatalloc(evaluate!, tensor, Rnl, Ylm)
   B = zeros(allocinfo...)
   return evaluate!(B, tensor, Rnl, Ylm)
end
=#

function evaluate(tensor::SparseACEbasis, Rnl, Ylm, ps, st)
   A = ka_evaluate(tensor.abasis, (Rnl, Ylm))
   AA = ka_evaluate(tensor.aabasis, A)
   # evaluate the coupling coefficients
   BB = tensor.A2Bmaps .* Ref(AA)
   return BB
end 

# for Ten3 inputs, there is no CPU implementation 
#   TODO (fix this!!!)

evaluate(tensor::SparseACEbasis, BB::TupTen3, args...) = 
      ka_evaluate(tensor, BB, args...)


evaluate(tensor::SparseACEbasis, Rnl::AbstractArray{T, 3}, Ylm::AbstractArray{T, 3}, 
         ps, st) where {T} = 
      ka_evaluate(tensor, Rnl, Ylm, ps, st)[1] 


# ---------


function pullback!(âˆ‚Rnl, âˆ‚Ylm, 
                   âˆ‚BB, tensor::SparseACEbasis, Rnl, Ylm, A)

   @no_escape begin 
   #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                           
   # âˆ‚Ei / âˆ‚AA = âˆ‚Ei / âˆ‚B * âˆ‚B / âˆ‚AA = (WB[i_z0]) * A2Bmap
   # âˆ‚AA = tensor.A2Bmap' * âˆ‚B   
   # T_âˆ‚AA = promote_type(eltype(âˆ‚B), eltype(tensor.A2Bmap))
   # âˆ‚AA = @alloc(T_âˆ‚AA, size(tensor.A2Bmap, 2))
   # mul!(âˆ‚AA, tensor.A2Bmap', âˆ‚B)
   # âˆ‚AA = tensor.A2Bmap' * âˆ‚B
   # T_âˆ‚AA = eltype(âˆ‚AA)
   # Dexuan's draft: 
   #  for (i, âˆ‚Báµ¢) in enumerate(âˆ‚BB)
   #      âˆ‚AA .+= tensor.A2Bmaps[i]' * âˆ‚Báµ¢
   #  end   
   âˆ‚AA = sum( tensor.A2Bmaps[i]' * âˆ‚BB[i] 
              for i = 1:length(âˆ‚BB) )
   T_âˆ‚AA = eltype(âˆ‚AA)

   # âˆ‚Ei / âˆ‚A = âˆ‚Ei / âˆ‚AA * âˆ‚AA / âˆ‚A = pullback(aabasis, âˆ‚AA)
   T_âˆ‚A = promote_type(T_âˆ‚AA, eltype(A))
   âˆ‚A = @alloc(T_âˆ‚A, length(tensor.abasis))
   pullback!(âˆ‚A, âˆ‚AA, tensor.aabasis, A)
   
   # âˆ‚Ei / âˆ‚Rnl, âˆ‚Ei / âˆ‚Ylm = pullback(abasis, âˆ‚A)
   pullback!((âˆ‚Rnl, âˆ‚Ylm), âˆ‚A, tensor.abasis, (Rnl, Ylm))

   #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   end # no_escape

   return âˆ‚Rnl, âˆ‚Ylm
end

function whatalloc(::typeof(pullback!),  
                   âˆ‚BB, tensor::SparseACEbasis, Rnl, Ylm
                   )
   # TODO: may need to check the type of âˆ‚BB too, but this is a bit 
   #       tricky because of the SVectors that can be in there...
   TB = eltype.(eltype.(âˆ‚BB))
   TA = promote_type(eltype(Rnl), eltype(Ylm), TB...)
   return (TA, size(Rnl)...), (TA, size(Ylm)...)
end

function pullback(âˆ‚BB, tensor::SparseACEbasis{T}, Rnl, Ylm, A) where {T}
   alc_âˆ‚Rnl, alc_âˆ‚Ylm = whatalloc(pullback!, âˆ‚BB, tensor, Rnl, Ylm)
   âˆ‚Rnl = zeros(alc_âˆ‚Rnl...)
   âˆ‚Ylm = zeros(alc_âˆ‚Ylm...)
   return pullback!(âˆ‚Rnl, âˆ‚Ylm, âˆ‚BB, tensor, Rnl, Ylm, A)
end


# ChainRules integration 
using ChainRulesCore: unthunk 

function rrule(::typeof(evaluate), tensor::SparseACEbasis, Rnl, Ylm, ps, st)

   # evaluate the A basis
   TA = promote_type(eltype(Rnl), eltype(eltype(Ylm)))
   A = zeros(TA, length(tensor.abasis))    # use Bumper here
   evaluate!(A, tensor.abasis, (Rnl, Ylm))

   # evaluate the AA basis
   AA = zeros(TA, length(tensor.aabasis))     # use Bumper here
   evaluate!(AA, tensor.aabasis, A)

   # evaluate the coupling coefficients
   BB = tensor.A2Bmaps .* Ref(AA)

   function pb(âˆ‚BB)
      âˆ‚Rnl, âˆ‚Ylm = pullback(unthunk.(âˆ‚BB), tensor, Rnl, Ylm, A)
      return NoTangent(), NoTangent(), âˆ‚Rnl, âˆ‚Ylm, ZeroTangent(), NoTangent() 
   end
   return BB, pb
end

# rrule for 3D array inputs (batched evaluation) - delegates to ka_evaluate
function rrule(::typeof(evaluate), tensor::SparseACEbasis,
               Rnl::Array{T, 3}, Ylm::Array{T, 3}, ps, st) where {T}
   # Delegate to ka_evaluate which has its own rrule
   ð”¹, A, ð”¸ = _ka_evaluate(tensor, Rnl, Ylm,
                          st.aspec, st.aaspecs, st.A2Bmaps)

   function pb_3d(âˆ‚out)
      âˆ‚ð”¹ = âˆ‚out[1]  # gradient w.r.t. ð”¹ (âˆ‚out[2] is for st which is NoTangent)
      âˆ‚Rnl, âˆ‚Ylm = _ka_pullback(âˆ‚ð”¹, tensor, Rnl, Ylm, A, ð”¸,
                                st.aspec, st.aaspecs, st.A2Bmaps)
      return NoTangent(), NoTangent(), âˆ‚Rnl, âˆ‚Ylm, NoTangent(), NoTangent()
   end

   return (ð”¹, st), pb_3d
end


# --------------------------------------------------------
# 
#  Jacobian of basis w.r.t. inputs (normally positions) 
#
# Assume the input data is organized as follows: 
#   Rnl : #j x #i x #R  array with #R the length of the radial basis 
#   Ylm : #j x #i x #Y  array with #Y the length of the spherical basis
#   dRnl, dYlm : same shape as Rnl, Ylm with 
#       dRnl[j, i, k] = âˆ‚Rnl[i, j] / âˆ‚X[i, j]  
#       dYlm[j, i, k] = âˆ‚Ylm[i, j] / âˆ‚X[i, j]
#

function _jacobian_X(tensor::SparseACEbasis, 
                     Rnl, Ylm, 
                     dRnl, dYlm)

   A, âˆ‚A = _jacobian_X(tensor.abasis, (Rnl, Ylm), (dRnl, dYlm))
   AA, âˆ‚AA = _jacobian_X(tensor.aabasis, A, âˆ‚A)
   
   # BB = tensor.A2Bmap * AA  if vector (single input)
   #     or AA * A2Bmap'  if matrix (batch)
   # BB = #nodes x #features 
   # âˆ‚BB = maxneigs x #nodes x #features
   BB = permutedims.( mul.(A2Bmaps, Ref(transpose(AA))) )

   # convert 3-tensor to matrix, apply A2Bmaps, then back to 3-tensor
   # this should be merged into a single kernel for efficiency 
   âˆ‚AA_mat = reshape(âˆ‚AA, :, size(âˆ‚AA, 3))
   âˆ‚BB_mat = permutedims.( mul.(A2Bmaps, Ref(transpose(AA))) )
   âˆ‚BB = reshape(âˆ‚BB_mat, size(âˆ‚AA, 1), :, size(âˆ‚BB_mat, 3))

   return BB, âˆ‚BB
end


# --------------------------------------------------------


const NT_NL_SPEC = NamedTuple{(:n, :l), Tuple{Int, Int}}

_nl(bb) = [(n = b.n, l = b.l) for b in bb]

function get_nnll_spec(tensor::SparseACEbasis{NL, TA, TAA, TSYM}, idx) where {NL, TA, TAA, TSYM}
   spec = tensor.meta["ð”¸spec"]::Vector{Vector{@NamedTuple{n::Int, l::Int, m::Int}}}
   A2Bmap = tensor.A2Bmaps[idx]
   nBB = size(A2Bmap, 1)
   nnll_list = Vector{Vector{NT_NL_SPEC}}(undef, nBB)
   for i in 1:nBB
      AAidx_nnz = A2Bmap[i, :].nzind
      bbs = spec[AAidx_nnz]
      nnll_list[i] = _nl(bbs[1])
   end
   return nnll_list
end

