
using LinearAlgebra, Lux, Random, EquivariantTensors, Test, Zygote
using ACEbase.Testing: print_tf, println_slim

import EquivariantTensors as ET 
import Polynomials4ML as P4ML      

include(joinpath(@__DIR__(), "..", "test_utils", "utils_gpu.jl"))
dev = gpu_device() 

##
# generate a model 
Dtot = 16   # total degree; specifies the trunction of embeddings and correlations
maxl = 10    # maximum degree of spherical harmonics 
ORD = 3     # correlation-order (body-order = ORD + 1)

# generate the embedding layer 
rbasis = ET.TransformedBasis( WrappedFunction(ğ« -> 1 / (1+norm(ğ«))), 
                              P4ML.ChebBasis(Dtot+1) )
ybasis = ET.TransformedBasis( WrappedFunction(ğ« -> ğ« / norm(ğ«)), 
                              P4ML.real_solidharmonics(maxl; T = Float32, static=true) )
embed = ET.ParallelEmbed(; Rnl = rbasis, Ylm = ybasis)

mb_spec = ET.sparse_nnll_set(; L = 0, ORD = ORD, 
                  minn = 0, maxn = Dtot, maxl = maxl, 
                  level = bb -> sum((b.n + b.l) for b in bb; init=0), 
                  maxlevel = Dtot)
ğ”¹basis = ET.sparse_equivariant_tensor(; 
            L = 0, mb_spec = mb_spec, 
            Rnl_spec = P4ML.natural_indices(rbasis.basis), 
            Ylm_spec = P4ML.natural_indices(ybasis.basis), 
            basis = real )

acel = ET.SparseACElayer(ğ”¹basis, (1,))

model = Lux.Chain(; embed = embed, ace = acel )
ps, st = LuxCore.setup(MersenneTwister(1234), model)
Î¸_0 = ps.ace.WLL[1]

##
# test evaluation 

# 1. generate a random input graph 
nnodes = 100
X = ET.Testing.rand_graph(nnodes; nneigrg = 10:20)

@info("Basic ETGraph tests")
print_tf(@test ET.nnodes(X) == nnodes)
print_tf(@test ET.maxneigs(X) <= 20)
print_tf(@test ET.nedges(X) == length(X.ii) == length(X.jj) == X.first[end] - 1)
print_tf(@test all( all(X.ii[X.first[i]:X.first[i+1]-1] .== i)
                    for i in 1:nnodes ) )

##
# 2. Move model and input to the GPU / Device 
ps_dev = dev(ps)
st_dev = dev(st)
X_dev = dev(X)

Ï†_dev, _ = model(X_dev, ps_dev, st_dev) 
Ï†_dev1 = Array(Ï†_dev[1])
Ï†, _ = model(X, ps, st) 
Ï†1 = Ï†[1]


## 
# now we try to make the same prediction with the original CPU ace 
# implementation, also skipping the graph datastructure entirely. 

function evaluate_env(model, ğ‘i)
   xij = [ rbasis.transin(ğ«, NamedTuple(), NamedTuple())[1] for ğ« in ğ‘i ]
   Rnl = P4ML.evaluate(rbasis.basis, xij, NamedTuple(), NamedTuple())
   ğ«Ì‚ij = [ ybasis.transin(ğ«, NamedTuple(), NamedTuple())[1] for ğ« in ğ‘i ]
   Ylm = P4ML.evaluate(ybasis.basis, ğ«Ì‚ij, NamedTuple(), NamedTuple())
   ğ”¹, = ET.evaluate(ğ”¹basis, Rnl, Ylm) 
   return dot(ğ”¹, Î¸_0)
end

@info("Test Old Sequential vs KA Evaluation")
Ï†_seq = [ evaluate_env(model, ET.neighbourhood(X, i)[2]) for i in 1:nnodes ]
println_slim(@test Ï†1 â‰ˆ Ï†_seq â‰ˆ Ï†_dev1) 

##

# This passes in interactive mode but fails in a CI/test run
# Ï†, âˆ‚X = ACEKA.evaluate_with_grad(model, X_dev, ps_dev, st_dev)

